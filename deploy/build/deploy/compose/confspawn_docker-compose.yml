services:
  # Service name, container name will be <project name>-<service name>-1 by default
  {{ db.service_name }}:
    container_name: "{{ db.container_name }}-${DEPLOY_NAME:-}"
    profiles: [ "data", "all" ]
    # Docker image that it will set up
    image: "${DB_APP_IMAGE}:${DB_VERSION}"
    # A volume is basically a file system shared between the host and the container
    volumes:
      # <volume name in volumes below>:<container destination directory>
      # This means the volume will be accessible from the container in the dest. directory in the container
      - {{ db.volume_name }}:${DB_RESOURCES_TARGET}
    command: -c config_file=${DB_CONF_FILE}
    # Some environment variables depend on deployment variables, so we load those in manually
    environment:
      - PGDATA=${DB_RESOURCES_TARGET}
      - POSTGRES_PASSWORD
      - POSTGRES_USER
    # This maps ports, so the container port will be available at localhost:<HOST_PORT>
    ports:
      - "${DB_HOST_HOST}:${DB_HOST_PORT}:{{ db.container_port }}"
    # Shared memory size, defaults to 64m which can cause problems (apparently)
    shm_size: 256m
    restart: {{ confspawn_env.restart }}
    healthcheck:
        test: "pg_isready -p {{ db.container_port }}"
        interval: 30s
        retries: 5
        start_period: 10s
        start_interval: 2s
        timeout: 10s
  {{ kv.service_name }}:
    container_name: "{{ kv.container_name }}-${DEPLOY_NAME:-}"
    profiles: [ "data", "all" ]
    image: "${KV_IMAGE}:${KV_VERSION}"
    environment:
      - REDIS_PASSWORD
    ports:
      - "${KV_HOST_HOST}:${KV_HOST_PORT}:{{ kv.container_port }}"
    restart: {{ confspawn_env.restart }}
  {{ server.service_name }}:
    container_name: "{{ server.container_name }}-${DEPLOY_NAME:-}"
    profiles: [ "all" ]
    depends_on:
      - {{ db.service_name }}
      - {{ kv.service_name }}
    # Docker image that it will set up
    image: "${SERVER_IMAGE}:${SERVER_VERSION}"
    # Some environment variables depend on deployment variables, so we load those in manually
    environment:
      DB_PASS: ${POSTGRES_PASSWORD:?err}
      KV_PASS: ${REDIS_PASSWORD:?err}
      MAIL_PASS: ${COMCOM_MAIL_PASS}
      KEY_PASS: ${KEY_PASSWORD:?err}
      RECREATE: ${RECREATE}
      DB_HOST: "${SERVER_DB_HOST}-${DEPLOY_NAME:-}"
      DB_PORT: ${SERVER_DB_PORT}
      KV_HOST: "${SERVER_KV_HOST}-${DEPLOY_NAME:-}"
      KV_PORT: ${SERVER_KV_PORT}
      MAIL_ENABLED: ${SERVER_MAIL_ENABLED}
      SMTP_SERVER: ${SERVER_SMTP_SERVER}
      SMTP_PORT: ${SERVER_SMTP_PORT}
    # This maps ports, so the container port will be available at localhost:<HOST_PORT>
    ports:
      - "${SERVER_HOST_HOST}:${SERVER_HOST_PORT}:{{ server.container_port }}"
    # Shared memory size, defaults to 64m which can cause problems (apparently)
    shm_size: 256m
    restart: {{ confspawn_env.restart }}
    # Prevent too large core dumps
    ulimits:
      core:
        soft: 0
        hard: 0
volumes:
  # This name should correspond to the volume mentioned in volumes above
  {{ db.volume_name }}:
    # A local directory
    driver: local
    # In general for staging this will be different in time as staging is always started with a specific version
    # In production this will in general be simply -production-latest
    name: {{ db.volume_name }}-${DEPLOY_NAME:-{{ confspawn_env.name }}}
networks:
  # Set up a network so that other containers on this network can access each other
  # A different container on this network can access this container by using the container name as the hostname
  # So localhost:3000 is exposed to other containers as <container name>:3000
  default:
    name: ${NETWORK_NAME}